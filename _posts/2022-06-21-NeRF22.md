---
title: 'NeRF at CVPR 2022'
date: 2022-06-21
last_modified_at: 2022-06-21
permalink: /NeRF22/
tags:
  - NeRF
  - Computer Vision
---

There are no less than 47 papers related to Neural Radiance Fields (NeRFs) at the [CVPR 2022](https://cvpr2022.thecvf.com/) conference, and probably more. With my former student and now colleague at Google Research, [Andrew Marmon](https://www.andrewmarmon.ai/), we rounded up all papers we could find and organized them here for our edification, and your reading pleasure.

Below are all the papers at CVPR'21 that we could find by scanning titles and reading the associated papers, sometimes rather superficially because of the sheer number. Please forgive any mis-characterizations and/or omissions, and feel free to flag them by DM to [@fdellaert](https://twitter.com/fdellaert) on twitter.

**Important note**: *all of the images below are reproduced from the cited papers, and the copyright belongs to the authors or the organization that published their papers, like IEEE.  Below I reproduce a key figure for some papers under the fair use clause of copyright law.* 

**This blog post is still a work in progress: I will add some images from selected papers over the next few days, and (maybe) a CVPR agenda.***

## NeRF 

NeRF was introduced in the seminal [Neural Radiance Fields paper](https://www.matthewtancik.com/nerf) by Mildenhall et al. at ECCV 2020.
By now NeRF is a phenomenon, but for those that are unfamiliar with it, please refer to the original paper or my two previous blog posts on the subject:
- [NeRF Explosion 2020](https://dellaert.github.io/NeRF/) 
- [NeRF at ICCV 2021](https://dellaert.github.io/NeRF21) 

In short, as shown in the figure below, a "vanilla" NeRF stores a volumetric scene representation as the weights of an MLP, trained on many images with known pose:

![NeRF overview](../images/NeRF/NeRF-setup.png)
*Figure: Nerf Overview*.

# Fundamentals

Again, many papers address the fundamentals of view-synthesis with NeRF-like methods:

<hr/>
<video width="100%" id="dollyzoom" autoplay="true" controls="" muted="" loop="" height="100%">
    <source src="https://bmild.github.io/rawnerf/img/rawnerf_16x9_showreel_crf23.mp4">
</video>

*Teaser videos from [NeRF in the Dark](https://bmild.github.io/rawnerf/index.html) (see below) which is just one of many papers that blew us away in terms of image synthesis quality.*.
<hr/>

[AR-NeRF](http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/ar-nerf/) replaces the pinhole-based ray tracing with aperture-based ray-tracing, enabling unsupervised learning of depth-of-field and defocus effects.

[Aug-NeRF](https://github.com/VITA-Group/Aug-NeRF) uses three different techniques to augment the training data to yield a significant boost in view synthesis quality.

[Deblur-NeRF](https://limacv.github.io/deblurnerf/) take an analysis-by-synthesis approach to recover a sharp NeRF from motion-blurred images, by simulating the blurring process using a learnable, spatially varying blur kernel.

[DIVeR](https://lwwu2.github.io/diver/) use a voxel-based representation to guide a *deterministic* volume rendering scheme, allowing it to render thin structures and other subtleties missed by traditional NeRF rendering.

[Ha-NeRFðŸ˜†](https://rover-xingyu.github.io/Ha-NeRF/) uses an appearance latent vector from images with different lighting and effects to render novel views with similarly-styled appearance.

[HDR-NeRF](https://shsf0817.github.io/hdr-nerf/) learns a separate MLP-based tone mapping function to transform the radiance and density of a given ray to a high-dynamic range (HDR) pixel color at that point in the output image.

[Mip-NeRF-360](https://jonbarron.info/mipnerf360/) extends the ICCV Mip-NeRF work to unbounded scenes, and also adds a prior that reduces cloudiness and other artifacts.

[NeRF in the Dark](https://bmild.github.io/rawnerf/index.html) modifies NeRF to train directly on raw images, and provide controls for HDR rendering including tone-mapping, focus, and exposure.

[NeRFReN](https://bennyguo.github.io/nerfren/) enables dealing with reflections by splitting a scene into transmitted and reflected components, and modeling the two components with separate neural radiance fields. 

[NeuRay](https://liuyuan-pal.github.io/NeuRay/) improves rendering quality by predicting the visibility of 3D points to input views, enabling the radiance field construction to focus on visible image features.

[Ref-NeRF](https://dorverbin.github.io/refnerf/) significantly improves the realism and accuracy of specular reflections by replacing NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance.

# Priors


One important way to improve the synthesis of new views instead is with various forms of generic or depth-driven priors:

[Dense Depth Priors for NeRF](https://barbararoessle.github.io/dense_depth_priors_nerf/) estimates depth using a depth completion network run on the SfM point cloud in order to constrain NeRF optimization,  yielding higher image quality on scenes with sparse input images.

[Depth-supervised NeRF](https://www.cs.cmu.edu/~dsnerf/) also uses a depth completion network on structure-from-motion point clouds to impose a depth-supervised loss for faster training time on fewer views of a given scene.

[InfoNeRF](http://cvlab.snu.ac.kr/research/InfoNeRF) penalizes the NeRF overfitting ray densities on scenes with limited input views through ray entropy regularization, resulting in higher quality depth maps when rendering novel views.

[RapNeRF](https://arxiv.org/abs/2205.05922) focuses on view-consistency to enable view extrapolation, using two new techniques: random ray casting and a ray atlas.

[RegNeRF](https://m-niemeyer.github.io/regnerf/) enables good reconstructions from a view images by renders patches in *unseen* views and 

# Multi-View


Another approach is to use nearby reference views at inference time, following a trend set by IBRNet and MVSNet:

[GeoNeRF](https://www.idiap.ch/paper/geonerf) uses feature-pyramid networks and homography warping to construct cascaded cost volumes on input views that infer local geometry and appearance on novel views, using a transformer-based approach.

[Light Field Neural Rendering](https://light-field-neural-rendering.github.io/) uses a lightfield parameterization for target pixel and its epipolar segments in nearby reference views, to produce high-quality renderings using a novel transformer architecture. 

[NAN](http://noise-aware-nerf.github.io/) builds upon IBRNet and NeRF to implement burst-denoising, now the standard way of coping with low-light imaging conditions.

[NeRFusion](https://jetd1.github.io/NeRFusion-Web/) first reconstructs local feature volumes for each view, using neighboring views, and then uses recurrent processing to construct a global neural volume.

# Performance


A big new trend is the emergence of voxel-based, very fast NeRF variants, many foregoing the large MLP at the center of the original NeRF paper:

[DVGO](https://sunset1995.github.io/dvgo/) replaces the large MLP with a voxel grid, directly storing opacity as well as local color features, interpolated and then fed into a small MLP to produce view-dependent color.

[EfficientNeRF](https://arxiv.org/abs/2206.00878) learns estimated object geometry from image features for efficient sampling around the surface of the object, reducing the time it takes to render and improving radiance field construction.

[Fourier PlenOctrees](https://aoliao12138.github.io/FPO/) tackles "efficient neural modeling and real-time rendering of dynamic scenes" using "Fourier PlenOctrees", achieving a 3000x speedup over NeRF.

[Plenoxels](https://alexyu.net/plenoxels) foregoes MLPs altogether and optimizes opacity and view-dependent color (using spherical harmonics) directly on a 3D voxel grid.

[Point-NeRF](https://xharlie.github.io/projects/project_sites/pointnerf) uses MVS techniques to obtain a dense point cloud, which is then used for per-point features, which are then fed to a (small) MLP for volume rendering.

# Large-scale


Large-scale scenes are also of intense interest, with various efforts in that dimension:

[Block-NeRF](http://waymo.com/research/block-nerf) scales NeRF to render city-scale scenes, decomposing the scene into individually trained NeRFs that are then combined to render the entire scene. Results are shown for 2.8M images.

[Mega-NeRF](https://meganerf.cmusatyalab.org/) decomposes a large scene into cells each with a separate NeRF, allowing for reconstructions of large scenes in significantly less time than previous approaches.

[Urban Radiance Fields](https://urban-radiance-fields.github.io/) allows for accurate 3D reconstruction of urban settings using panoramas and lidar information by compensating for photometric effects and supervising model training with lidar-based depth

# Articulated


A second emerging trend is the application of neural radiance field for articulated models of people:

[BANMo](https://banmo-www.github.io/) combines deformable shape models, canonical embeddings, and NeRF-style volume rendering to train high-fidelity, articulated 3D models from many casual RGB videos.

[DoubleField](http://www.liuyebin.com/dbfield/dbfield.html) trains a surface field as well as radiance field, using a shared feature embedding,  to allow for high-fidelity human reconstruction and rendering on limited input views.

[HumanNeRF](https://grail.cs.washington.edu/projects/humannerf/) optimizes for a volumetric representation of a person in a canonical pose, and estimates a motion field for every frame with non-rigid and skeletal components.

[HumanNeRF (2)](https://zhaofuq.github.io/humannerf/) estimates human geometry and appearance through a dynamic NeRF approach along with a neural appearance blending model from adjacent views to create dynamic free-viewpoint video using as few as six input views.

[NeuralHOFusion](https://nowheretrix.github.io/neuralfusion/) learns separate human and object models from a sparse number of input masks extracted from RGBD images, resulting in realistic free-viewpoint videos despite occlusions and challenging poses.

[Structured Local Radiance Fields](https://liuyebin.com/slrf/slrf.html) uses pose estimation to build a set of local radiance fields specific to nodes on an SMPL model which, when combined with an appearance embedding, yields realistic 3D animations.

[Surface-Aligned NeRF](https://pfnet-research.github.io/surface-aligned-nerf/) maps a query coordinate to its dispersed projection point on a pre-defined human mesh, using the mesh itself and the view direction to be input to the NeRF for high-quality dynamic rendering.

# Portrait


Some papers are focused on the generation of controllable face images and/or 3D head models:

[FENeRF](https://mrtornado24.github.io/FENeRF/) learns a 3D-aware human face representation with two latent codes, which can generate editable and view-consistent 2D face images.

[GRAM](https://yudeng.github.io/GRAM/) uses a separate manifold predictor network to constrain the volume rendering samples near the surface, yielding high-quality results with fine details.

[HeadNeRF](https://hy1995.top/HeadNeRF-Project/) integrates 2D rendering into the NeRF rendering process for rendering controllable avatars at 40 fps.

[RigNeRF](https://arxiv.org/abs/2206.06481) enables full control of head pose and facial expressions learned from a
single portrait video by using a deformation field that is guided
by a 3D morphable face model. 

# Editable


Controllable or *editable* NerFs are closely related:

[CLIP-NeRF](https://cassiepython.github.io/clipnerf/) supports editing a conditional model using text or image guidance via their CLIP embeddings.

[CoNeRF](https://conerf.github.io/) takes a single video, along with some attribute annotations, and allow re-rendering while controlling the attributes independently, along with viewpoint.

[NeRF-Editing](https://arxiv.org/abs/2205.04978) allows for editing of a reconstructed mesh output from NeRF by creating a continuous deformation field around edited components to bend the direction of the rays according to its updated geometry.

# Conditional


Continuing a trend started at ICCV is conditioning NeRF-like models on various latent codes:

[ðŸ¤£LOLNeRF](https://ubc-vision.github.io/lolnerf/) uses pose estimation and segmentation techniques to train a conditional NeRF on single views, which then at inference time can generate different faces with the same pose, or one face in different poses.

[Pix2NeRF](https://github.com/primecai/Pix2NeRF) extends Ï€-GAN with an encoder, trained jointly with the GAN, to allow mapping images back to a latent manifold, allowing for object-centric novel view synthesis using a single input image.

[StylizedNeRF](http://geometrylearning.com/StylizedNeRF/) "pre-train a standard NeRF of the 3D scene to be stylized and replace its color prediction module with a style network to obtain a stylized NeRF."

# Composition


Finally, close to my interests, compositional approaches that use object-like priors:

[AutoRF](https://sirwyver.github.io/AutoRF/) learns appearance and shape priors for a given class of objects to enable single-shot reconstruction for novel view synthesis.

[PNF](https://abhijitkundu.info/projects/pnf/) fits a separate NeRF to individual object instances, creating a panoptic-radiance field that can render dynamic scenes by composing multiple instance-NeRFs and a single "stuff"-NeRF.


# Concluding Thoughts

I am happy that with [Panoptic Neural Fields](https://abhijitkundu.info/projects/pnf/) I am finally myself a co-author on a NerF paper, but this is probably the last of these blog posts I will write: it is getting too hard to keep track of all the papers in this space, and growth seems exponential. It is increasingly hard, as well, to come up with ideas in this space without being scooped: I myself was scooped after some months of work on an idea, and I know of many others that found themselves in the same boat. Nevertheless, it is an exciting time to be in 3D computer vision, and I am excited to see what the future will bring.