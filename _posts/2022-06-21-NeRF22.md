---
title: 'NeRF at CVPR 2022'
date: 2022-06-21
last_modified_at: 2022-06-21
permalink: /NeRF22/
tags:
  - NeRF
  - Computer Vision
---

There are more than 50 papers related to Neural Radiance Fields (NeRFs) at the [CVPR 2022](https://cvpr2022.thecvf.com/) conference. With my former student and now colleague at Google Research, [Andrew Marmon](https://www.andrewmarmon.ai/), we rounded up all papers we could find and organized them here for our edification, and your reading pleasure.

Below are all the papers at CVPR'22 that we could find by scanning titles and reading the associated papers, sometimes rather superficially because of the sheer number. Please forgive any mis-characterizations and/or omissions, and feel free to flag them by DM to [@fdellaert](https://twitter.com/fdellaert) on twitter.

**Important note**: *all of the images below are reproduced from the cited papers, and the copyright belongs to the authors or the organization that published their papers, like IEEE.  Below I reproduce a key figure or video for some papers under the fair use clause of copyright law.* 

## NeRF 

NeRF was introduced in the seminal [Neural Radiance Fields paper](https://www.matthewtancik.com/nerf) by Mildenhall et al. at ECCV 2020.
By now NeRF is a phenomenon, but for those that are unfamiliar with it, please refer to the original paper or my two previous blog posts on the subject:
- [NeRF Explosion 2020](https://dellaert.github.io/NeRF/) 
- [NeRF at ICCV 2021](https://dellaert.github.io/NeRF21) 

In short, as shown in the figure below, a "vanilla" NeRF stores a volumetric scene representation as the weights of an MLP, trained on many images with known pose:

![NeRF overview](../images/NeRF/NeRF-setup.png)
*Figure: Nerf Overview*.

# Fundamentals

Again, many papers address the fundamentals of view-synthesis with NeRF-like methods:

<hr/>
<video width="100%" id="dollyzoom" autoplay="true" controls="" muted="" loop="" height="100%">
    <source src="https://bmild.github.io/rawnerf/img/rawnerf_16x9_showreel_crf23.mp4">
</video>

*Teaser videos from [NeRF in the Dark](https://bmild.github.io/rawnerf/index.html) (see below) which is just one of many papers that blew us away in terms of image synthesis quality*.
<hr/>

[AR-NeRF](http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/ar-nerf/) replaces the pinhole-based ray tracing with aperture-based ray-tracing, enabling unsupervised learning of depth-of-field and defocus effects. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Kaneko_AR-NeRF_Unsupervised_Learning_of_Depth_and_Defocus_Effects_From_Natural_CVPR_2022_paper.pdf)

[Aug-NeRF](https://github.com/VITA-Group/Aug-NeRF) uses three different techniques to augment the training data to yield a significant boost in view synthesis quality. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Aug-NeRF_Training_Stronger_Neural_Radiance_Fields_With_Triple-Level_Physically-Grounded_Augmentations_CVPR_2022_paper.pdf)

[Deblur-NeRF](https://limacv.github.io/deblurnerf/) take an analysis-by-synthesis approach to recover a sharp NeRF from motion-blurred images, by simulating the blurring process using a learnable, spatially varying blur kernel. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Deblur-NeRF_Neural_Radiance_Fields_From_Blurry_Images_CVPR_2022_paper.pdf)

[DIVeR](https://lwwu2.github.io/diver/) use a voxel-based representation to guide a *deterministic* volume rendering scheme, allowing it to render thin structures and other subtleties missed by traditional NeRF rendering. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_DIVeR_Real-Time_and_Accurate_Neural_Radiance_Fields_With_Deterministic_Integration_CVPR_2022_paper.pdf) **Best Paper Finalist**

[Ha-NeRFðŸ˜†](https://rover-xingyu.github.io/Ha-NeRF/) uses an appearance latent vector from images with different lighting and effects to render novel views with similarly-styled appearance. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Hallucinated_Neural_Radiance_Fields_in_the_Wild_CVPR_2022_paper.pdf)

[HDR-NeRF](https://shsf0817.github.io/hdr-nerf/) learns a separate MLP-based tone mapping function to transform the radiance and density of a given ray to a high-dynamic range (HDR) pixel color at that point in the output image. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Huang_HDR-NeRF_High_Dynamic_Range_Neural_Radiance_Fields_CVPR_2022_paper.pdf)

[Learning Neural Light Fields](https://neural-light-fields.github.io/) learn a 4D lightfield, but transform the 4D input to an embedding space first to enable generalization from sparse 4D training samples, which gives good view dependent results. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Attal_Learning_Neural_Light_Fields_With_Ray-Space_Embedding_CVPR_2022_paper.pdf)

[Mip-NeRF-360](https://jonbarron.info/mipnerf360/) extends the ICCV Mip-NeRF work to unbounded scenes, and also adds a prior that reduces cloudiness and other artifacts. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Barron_Mip-NeRF_360_Unbounded_Anti-Aliased_Neural_Radiance_Fields_CVPR_2022_paper.pdf)

[NeRF in the Dark](https://bmild.github.io/rawnerf/index.html) modifes NeRF to train directly on raw images, and provide controls for HDR rendering including tone-mapping, focus, and exposure. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Mildenhall_NeRF_in_the_Dark_High_Dynamic_Range_View_Synthesis_From_CVPR_2022_paper.pdf)

[NeRFReN](https://bennyguo.github.io/nerfren/) enables dealing with reflections by splitting a scene into transmitted and reflected components, and modeling the two components with separate neural radiance fields.  [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_NeRFReN_Neural_Radiance_Fields_With_Reflections_CVPR_2022_paper.pdf)

[NeuRay](https://liuyuan-pal.github.io/NeuRay/) improves rendering quality by predicting the visibility of 3D points to input views, enabling the radiance field construction to focus on visible image features. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Neural_Rays_for_Occlusion-Aware_Image-Based_Rendering_CVPR_2022_paper.pdf)

[Ref-NeRF](https://dorverbin.github.io/refnerf/) significantly improves the realism and accuracy of specular reflections by replacing NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Verbin_Ref-NeRF_Structured_View-Dependent_Appearance_for_Neural_Radiance_Fields_CVPR_2022_paper.pdf)  **Best Student Paper Honorable Mention**

[SRT](https://srt-paper.github.io/) "processes posed or unposed RGB images of a new area, infers a 'set-latent scene representation', and synthesizes novel views, all in a single feed-forward pass." [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Sajjadi_Scene_Representation_Transformer_Geometry-Free_Novel_View_Synthesis_Through_Set-Latent_Scene_CVPR_2022_paper.pdf)


# Priors

One important way to improve the synthesis of new views instead is with various forms of generic or depth-driven priors:

<hr/>

![Dense Depth Priors for NeRF estimates depth using a depth completion network run on the SfM point cloud in order to constrain NeRF optimization,  yielding higher image quality on scenes with sparse input images](https://barbararoessle.github.io/dense_depth_priors_nerf/static/images/pipeline.jpg)
*Figure: Dense Depth Priors for NeRF*
<hr/>
[Dense Depth Priors for NeRF](https://barbararoessle.github.io/dense_depth_priors_nerf/) estimates depth using a depth completion network run on the SfM point cloud in order to constrain NeRF optimization,  yielding higher image quality on scenes with sparse input images. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Roessle_Dense_Depth_Priors_for_Neural_Radiance_Fields_From_Sparse_Input_CVPR_2022_paper.pdf)

[Depth-supervised NeRF](https://www.cs.cmu.edu/~dsnerf/) also uses a depth completion network on structure-from-motion point clouds to impose a depth-supervised loss for faster training time on fewer views of a given scene. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Deng_Depth-Supervised_NeRF_Fewer_Views_and_Faster_Training_for_Free_CVPR_2022_paper.pdf)

[InfoNeRF](http://cvlab.snu.ac.kr/research/InfoNeRF) penalizes the NeRF overfitting ray densities on scenes with limited input views through ray entropy regularization, resulting in higher quality depth maps when rendering novel views. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_InfoNeRF_Ray_Entropy_Minimization_for_Few-Shot_Neural_Volume_Rendering_CVPR_2022_paper.pdf)

[RapNeRF](https://arxiv.org/abs/2205.05922) focuses on view-consistency to enable view extrapolation, using two new techniques: random ray casting and a ray atlas. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Ray_Priors_Through_Reprojection_Improving_Neural_Radiance_Fields_for_Novel_CVPR_2022_paper.pdf)

[RegNeRF](https://m-niemeyer.github.io/regnerf/) enables good reconstructions from a view images by renders patches in *unseen* views and minimizing an appearance and depth smoothness prior there.  [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Niemeyer_RegNeRF_Regularizing_Neural_Radiance_Fields_for_View_Synthesis_From_Sparse_CVPR_2022_paper.pdf)

# Multi-View

Another approach is to use nearby reference views at inference time, following a trend set by IBRNet and MVSNet:

<hr/>
<video width="100%" id="dollyzoom" autoplay="true" controls="" muted="" loop="" height="100%">
    <source src="https://light-field-neural-rendering.github.io/static/videos/shiny_lab.mp4">
</video>

*Result from from [Light Field Neural Rendering](https://light-field-neural-rendering.github.io/) (see below) which uses nearby views and a light-field parameterization to render very non-trivial effects*.
<hr/>


[GeoNeRF](https://www.idiap.ch/paper/geonerf) uses feature-pyramid networks and homography warping to construct cascaded cost volumes on input views that infer local geometry and appearance on novel views, using a transformer-based approach. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Johari_GeoNeRF_Generalizing_NeRF_With_Geometry_Priors_CVPR_2022_paper.pdf)

[Light Field Neural Rendering](https://light-field-neural-rendering.github.io/) uses a lightfield parameterization for target pixel and its epipolar segments in nearby reference views, to produce high-quality renderings using a novel transformer architecture.  [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Suhail_Light_Field_Neural_Rendering_CVPR_2022_paper.pdf) **Best Paper Finalist**

[NAN](http://noise-aware-nerf.github.io/) builds upon IBRNet and NeRF to implement burst-denoising, now the standard way of coping with low-light imaging conditions. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Pearl_NAN_Noise-Aware_NeRFs_for_Burst-Denoising_CVPR_2022_paper.pdf)

[NeRFusion](https://jetd1.github.io/NeRFusion-Web/) first reconstructs local feature volumes for each view, using neighboring views, and then uses recurrent processing to construct a global neural volume. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_NeRFusion_Fusing_Radiance_Fields_for_Large-Scale_Scene_Reconstruction_CVPR_2022_paper.pdf)

# Performance

A big new trend is the emergence of voxel-based, very fast NeRF variants, many foregoing the large MLP at the center of the original NeRF paper:

<hr/>
<video width="100%" id="dollyzoom" autoplay="true" controls="" muted="" loop="" height="100%">
    <source src="https://people.eecs.berkeley.edu/~kanazawa/cachedir/plenoxel_data/fastopt.mp4">
</video>

*[Plenoxels](https://alexyu.net/plenoxels) (see below) is one of the no-MLP papers that took the NeRF community by storm. DVGO (also below) and [instant NGP method](https://nvlabs.github.io/instant-ngp/), published not at CVPR but at SIGGRAPH 22, are other papers in this space. Goodbye long training times?*
<hr/>

[DVGO](https://sunset1995.github.io/dvgo/) replaces the large MLP with a voxel grid, directly storing opacity as well as local color features, interpolated and then fed into a small MLP to produce view-dependent color. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_Direct_Voxel_Grid_Optimization_Super-Fast_Convergence_for_Radiance_Fields_Reconstruction_CVPR_2022_paper.pdf)

[EfficientNeRF](https://arxiv.org/abs/2206.00878) learns estimated object geometry from image features for efficient sampling around the surface of the object, reducing the time it takes to render and improving radiance field construction. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Hu_EfficientNeRF__Efficient_Neural_Radiance_Fields_CVPR_2022_paper.pdf)

[Fourier PlenOctrees](https://aoliao12138.github.io/FPO/) tackles "efficient neural modeling and real-time rendering of dynamic scenes" using "Fourier PlenOctrees", achieving a 3000x speedup over NeRF. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Fourier_PlenOctrees_for_Dynamic_Radiance_Field_Rendering_in_Real-Time_CVPR_2022_paper.pdf)

[Plenoxels](https://alexyu.net/plenoxels) foregoes MLPs altogether and optimizes opacity and view-dependent color (using spherical harmonics) directly on a 3D voxel grid. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Fridovich-Keil_Plenoxels_Radiance_Fields_Without_Neural_Networks_CVPR_2022_paper.pdf)

[Point-NeRF](https://xharlie.github.io/projects/project_sites/pointnerf) uses MVS techniques to obtain a dense point cloud, which is then used for per-point features, which are then fed to a (small) MLP for volume rendering. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Point-NeRF_Point-Based_Neural_Radiance_Fields_CVPR_2022_paper.pdf)

# Large-scale

Large-scale scenes are also of intense interest, with various efforts in that dimension:

<hr/>
<video width="100%" id="dollyzoom" autoplay="true" controls="" muted="" loop="" height="100%">
    <source src="https://storage.googleapis.com/waymo-uploads/files/site-media/research/waymo_block_nerf_grace_cathedral.mp4">
</video>

*[Block-NeRF](http://waymo.com/research/block-nerf) (see below) shows view synthesis derived from 2.8 million images.*
<hr/>

[Block-NeRF](http://waymo.com/research/block-nerf) scales NeRF to render city-scale scenes, decomposing the scene into individually trained NeRFs that are then combined to render the entire scene. Results are shown for 2.8M images. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Tancik_Block-NeRF_Scalable_Large_Scene_Neural_View_Synthesis_CVPR_2022_paper.pdf)

[Mega-NeRF](https://meganerf.cmusatyalab.org/) decomposes a large scene into cells each with a separate NeRF, allowing for reconstructions of large scenes in significantly less time than previous approaches. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Turki_Mega-NERF_Scalable_Construction_of_Large-Scale_NeRFs_for_Virtual_Fly-Throughs_CVPR_2022_paper.pdf)


[Urban Radiance Fields](https://urban-radiance-fields.github.io/) allows for accurate 3D reconstruction of urban settings using panoramas and lidar information by compensating for photometric effects and supervising model training with lidar-based depth. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Rematas_Urban_Radiance_Fields_CVPR_2022_paper.pdf)

# Articulated

A second emerging trend is the application of neural radiance field for articulated models of people, or cats ðŸ˜Š:

<hr/>

![BANMo](https://banmo-www.github.io/vids/teaser-small.gif)

*[BANMo](https://banmo-www.github.io/) (see below) creates a deformable NeRF from your cat videos!*
<hr/>

[BANMo](https://banmo-www.github.io/) combines deformable shape models, canonical embeddings, and NeRF-style volume rendering to train high-fidelity, articulated 3D models from many casual RGB videos. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_BANMo_Building_Animatable_3D_Neural_Models_From_Many_Casual_Videos_CVPR_2022_paper.pdf)

[DoubleField](http://www.liuyebin.com/dbfield/dbfield.html) trains a surface field as well as radiance field, using a shared feature embedding,  to allow for high-fidelity human reconstruction and rendering on limited input views. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Shao_DoubleField_Bridging_the_Neural_Surface_and_Radiance_Fields_for_High-Fidelity_CVPR_2022_paper.pdf)

[HumanNeRF](https://grail.cs.washington.edu/projects/humannerf/) optimizes for a volumetric representation of a person in a canonical pose, and estimates a motion field for every frame with non-rigid and skeletal components. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Weng_HumanNeRF_Free-Viewpoint_Rendering_of_Moving_People_From_Monocular_Video_CVPR_2022_paper.pdf)

[HumanNeRF (2)](https://zhaofuq.github.io/humannerf/) estimates human geometry and appearance through a dynamic NeRF approach along with a neural appearance blending model from adjacent views to create dynamic free-viewpoint video using as few as six input views. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_HumanNeRF_Efficiently_Generated_Human_Radiance_Field_From_Sparse_Inputs_CVPR_2022_paper.pdf)

[NeuralHOFusion](https://nowheretrix.github.io/neuralfusion/) learns separate human and object models from a sparse number of input masks extracted from RGBD images, resulting in realistic free-viewpoint videos despite occlusions and challenging poses. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_NeuralHOFusion_Neural_Volumetric_Rendering_Under_Human-Object_Interactions_CVPR_2022_paper.pdf)

[Structured Local Radiance Fields](https://liuyebin.com/slrf/slrf.html) uses pose estimation to build a set of local radiance fields specific to nodes on an SMPL model which, when combined with an appearance embedding, yields realistic 3D animations. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_Structured_Local_Radiance_Fields_for_Human_Avatar_Modeling_CVPR_2022_paper.pdf)

[Surface-Aligned NeRF](https://pfnet-research.github.io/surface-aligned-nerf/) maps a query coordinate to its dispersed projection point on a pre-defined human mesh, using the mesh itself and the view direction to be input to the NeRF for high-quality dynamic rendering. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Surface-Aligned_Neural_Radiance_Fields_for_Controllable_3D_Human_Synthesis_CVPR_2022_paper.pdf)

[VEOs](https://www.youtube.com/watch?v=GJPdhtQ_-K4) use a multi-view variant of non-rigid NeRF for object reconstruction and tracking of plushy objects, which can then be rendered in new deformed states. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Virtual_Elastic_Objects_CVPR_2022_paper.pdf)

# Portrait

Some papers are focused on the generation of controllable face images and/or 3D head models for people, *and* cats:

<hr/>
<video width="100%" id="dollyzoom" autoplay="true" controls="" muted="" loop="" height="100%">
    <source src="https://yudeng.github.io/GRAM/files/teaser.mp4">
</video>

*[GRAM](https://yudeng.github.io/GRAM/) (see below) focuses its radiance fields to be sampled near the surface for some amazing results.*
<hr/>

[EG3D](https://nvlabs.github.io/eg3d/) is a geometry-aware GAN that uses a novel tri-plane volumetric representation (somewhere between implicit and voxels) to allow for real-time rendering to a low-res image, upscaled via super-resolution. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Chan_Efficient_Geometry-Aware_3D_Generative_Adversarial_Networks_CVPR_2022_paper.pdf)

[FENeRF](https://mrtornado24.github.io/FENeRF/) learns a 3D-aware human face representation with two latent codes, which can generate editable and view-consistent 2D face images. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_FENeRF_Face_Editing_in_Neural_Radiance_Fields_CVPR_2022_paper.pdf)

[GRAM](https://yudeng.github.io/GRAM/) uses a separate manifold predictor network to constrain the volume rendering samples near the surface, yielding high-quality results with fine details. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Deng_GRAM_Generative_Radiance_Manifolds_for_3D-Aware_Image_Generation_CVPR_2022_paper.pdf)

[HeadNeRF](https://hy1995.top/HeadNeRF-Project/) integrates 2D rendering into the NeRF rendering process for rendering controllable avatars at 40 fps. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Hong_HeadNeRF_A_Real-Time_NeRF-Based_Parametric_Head_Model_CVPR_2022_paper.pdf)

[RigNeRF](https://arxiv.org/abs/2206.06481) enables full control of head pose and facial expressions learned from a
single portrait video by using a deformation field that is guided
by a 3D morphable face model.  [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Athar_RigNeRF_Fully_Controllable_Neural_3D_Portraits_CVPR_2022_paper.pdf)

[StyleSDF](https://stylesdf.github.io/) combines a conditional SDFNet, a Nerf-style volume renderer, and a 2D style-transfer network to generate high quality face models/images. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Or-El_StyleSDF_High-Resolution_3D-Consistent_Image_and_Geometry_Generation_CVPR_2022_paper.pdf)


# Editable

Controllable or *editable* NerFs are closely related:

<hr/>

![CLIP-NeRF supports editing a conditional model using text or image guidance via their CLIP embeddings.](https://cassiepython.github.io/clipnerf/images/teaser.gif)

*With [CLIP-NeRF](https://cassiepython.github.io/clipnerf/) (see below) you can edit NeRFs with textual guidance, or example images.*
<hr/>

[CLIP-NeRF](https://cassiepython.github.io/clipnerf/) supports editing a conditional model using text or image guidance via their CLIP embeddings. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_CLIP-NeRF_Text-and-Image_Driven_Manipulation_of_Neural_Radiance_Fields_CVPR_2022_paper.pdf)

[CoNeRF](https://conerf.github.io/) takes a single video, along with some attribute annotations, and allow re-rendering while controlling the attributes independently, along with viewpoint. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Kania_CoNeRF_Controllable_Neural_Radiance_Fields_CVPR_2022_paper.pdf)

[NeRF-Editing](https://arxiv.org/abs/2205.04978) allows for editing of a reconstructed mesh output from NeRF by creating a continuous deformation field around edited components to bend the direction of the rays according to its updated geometry. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Yuan_NeRF-Editing_Geometry_Editing_of_Neural_Radiance_Fields_CVPR_2022_paper.pdf)


# Conditional

Continuing a trend started at ICCV is conditioning NeRF-like models on various latent codes:

[ðŸ¤£LOLNeRF](https://ubc-vision.github.io/lolnerf/) uses pose estimation and segmentation techniques to train a conditional NeRF on single views, which then at inference time can generate different faces with the same pose, or one face in different poses. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Rebain_LOLNerf_Learn_From_One_Look_CVPR_2022_paper.pdf)

[Pix2NeRF](https://github.com/primecai/Pix2NeRF) extends Ï€-GAN with an encoder, trained jointly with the GAN, to allow mapping images back to a latent manifold, allowing for object-centric novel view synthesis using a single input image. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Cai_Pix2NeRF_Unsupervised_Conditional_p-GAN_for_Single_Image_to_Neural_Radiance_CVPR_2022_paper.pdf)

[StylizedNeRF](http://geometrylearning.com/StylizedNeRF/) "pre-train a standard NeRF of the 3D scene to be stylized and replace its color prediction module with a style network to obtain a stylized NeRF." [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Huang_StylizedNeRF_Consistent_3D_Scene_Stylization_As_Stylized_NeRF_via_2D-3D_CVPR_2022_paper.pdf)

# Composition

Close to my interests, compositional approaches that use object-like priors:

<hr/>
<video width="100%" id="dollyzoom" autoplay="true" controls="" muted="" loop="" height="100%">
    <source src="https://abhijitkundu.info/assets/videos/pnf/pnf_applications.mp4">
</video>

*[Panoptic Neural Fields (PNF)](https://abhijitkundu.info/projects/pnf/) (see below) has many object-NeRFs and a "stuff"-NeRF, supporting many different synthesis outputs.*
<hr/>

[AutoRF](https://sirwyver.github.io/AutoRF/) learns appearance and shape priors for a given class of objects to enable single-shot reconstruction for novel view synthesis. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Muller_AutoRF_Learning_3D_Object_Radiance_Fields_From_Single_View_Observations_CVPR_2022_paper.pdf)

[PNF](https://abhijitkundu.info/projects/pnf/) fits a separate NeRF to individual object instances, creating a panoptic-radiance field that can render dynamic scenes by composing multiple instance-NeRFs and a single "stuff"-NeRF. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Kundu_Panoptic_Neural_Fields_A_Semantic_Object-Aware_Neural_Scene_Representation_CVPR_2022_paper.pdf)

# Other

Finally, several different (and pretty cool!) applications of NeRF:

<hr/>

![DyNeRF uses compact latent codes to represent the frames in a 3D video and is able to render the scene from free viewpoints, with impressive volumetric rendering effects.](https://lvzhaoyang.github.io/images/n3d_martini.gif)

*[DyNeRF](https://neural-3d-video.github.io/) (see below) allows free-viewpoint re-rendering of a video once latent descriptions for all frames have been learned.*
<hr/>


[Dream Fields](https://ajayj.com/dreamfields) synthesizes a NeRF from a text caption alone, minimizing a CLIP-based loss as well as regularizing transmittance to reduce artifacts. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Jain_Zero-Shot_Text-Guided_Object_Generation_With_Dream_Fields_CVPR_2022_paper.pdf)

[DyNeRF](https://neural-3d-video.github.io/) uses compact latent codes to represent the frames in a 3D video and is able to render the scene from free viewpoints, with impressive volumetric rendering effects. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/html/Li_Neural_3D_Video_Synthesis_From_Multi-View_Video_CVPR_2022_paper.html)

[Kubric](https://kubric.readthedocs.io/en/latest/) is not really a NeRF paper but provides "an open-source Python framework that interfaces with PyBullet and Blender to generate photo-realistic scenes" that can directly provide training data to NeRF pipelines. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Greff_Kubric_A_Scalable_Dataset_Generator_CVPR_2022_paper.pdf)

[NICE_SLAM](https://pengsongyou.github.io/nice-slam) uses a hierarchical voxel-grid NeRF variant to render RGBD, for a real-time and scalable parallel tracking and mapping dense SLAM system for RGBD inputs. [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_NICE-SLAM_Neural_Implicit_Scalable_Encoding_for_SLAM_CVPR_2022_paper.pdf)

[STEM-NeRF](https://www.uni-ulm.de/in/mi/mi-forschung/viscom/publikationen?category=publication&publication_id=232) use a differentiable image formation model for Scanning Transmission Electron Microscopes (STEMs) [(pdf)](https://openaccess.thecvf.com/content/CVPR2022/papers/Kniesel_Clean_Implicit_3D_Structure_From_Noisy_2D_STEM_Images_CVPR_2022_paper.pdf)

# Concluding Thoughts

I am happy that with [Panoptic Neural Fields](https://abhijitkundu.info/projects/pnf/) I am finally myself a co-author on a NerF paper, but this is probably the last of these blog posts I will write: it is getting too hard to keep track of all the papers in this space, and growth seems exponential. It is increasingly hard, as well, to come up with ideas in this space without being scooped: I myself was scooped after some months of work on an idea, and I know of many others that found themselves in the same boat. Nevertheless, it is an exciting time to be in 3D computer vision, and I am excited to see what the future will bring.